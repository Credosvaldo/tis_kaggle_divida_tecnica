{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n## <b>1 <span style='color:#3f4d63'>I</span> Introduction</b>\n---\n\nThis notebook corresponds to the 2nd part of the whole project I'm developing for this competition. **In 1st part I did a complete EDA of the data we're given. If you want to regard it, you'll find it here** -> [English language learning | Complete EDA](https://www.kaggle.com/code/javigallego/english-language-learning-complete-eda). \n\nIf you want to just examine model's architecture, please head into [Model's Architecture]() subsection.\n\n#### **Importing packages ⬇️**","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output, display_html\nimport gc; gc.enable()\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\nimport warnings\nfrom pathlib import Path\n\n# Basic libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy as sc\nfrom scipy import stats\n\n# Train Test Split\nfrom sklearn.model_selection import train_test_split\n\n# Cross Validation\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, learning_curve, train_test_split\n\n# Tensorflow\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Plotly\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\nimport plotly.graph_objs as go\n\nwarnings.filterwarnings('ignore')\n\ndef load_data():\n    '''Load each of the datasets we are given.'''\n    \n    data_dir = Path(\"../input/feedback-prize-english-language-learning\")\n    train = pd.read_csv(data_dir / \"train.csv\")\n    test = pd.read_csv(data_dir / \"test.csv\")\n    sample_submission = pd.read_csv(data_dir / 'sample_submission.csv')\n    return train, test, sample_submission\n\ntrain, test, sample_submission = load_data()\nclear_output()","metadata":{"_uuid":"9820b3d3-10b9-40d5-ab15-4a7cfb0ba4ef","_cell_guid":"f8ab7749-9210-4160-b27a-6188dbf0fc83","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-10-05T19:01:45.27078Z","iopub.execute_input":"2022-10-05T19:01:45.271298Z","iopub.status.idle":"2022-10-05T19:01:55.388965Z","shell.execute_reply.started":"2022-10-05T19:01:45.271197Z","shell.execute_reply":"2022-10-05T19:01:55.387721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](data:image/png;base64,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)\n\n### **Weights and Biases ⬇️**\n\nYou will need a unique API key to log in to Weights & Biases.\n\n* If you don't have a Weights & Biases account, you can go to [https://wandb.ai/site](https://wandb.ai/site) and create a FREE account. Access your API key: [https://wandb.ai/authorize](https://wandb.ai/authorize).\n\nThere are two ways you can login using a Kaggle kernel:\n\n* Run a cell with `wandb.login()`. It will ask for the API key, which you can `copy` + `paste` in.\n* You can also use Kaggle secrets to store your API key and use the code snippet below to login. Check out [this discussion post](https://www.kaggle.com/product-feedback/114053) to learn more about Kaggle secrets.","metadata":{}},{"cell_type":"code","source":"!pip install wandb\nclear_output()\nimport wandb\nfrom wandb.keras import WandbCallback\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"_uuid":"d569f545-6625-463f-81bb-88426e51db56","_cell_guid":"71f254e2-d202-4cd5-bd03-fc6afbc5f454","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-05T19:01:59.886467Z","iopub.execute_input":"2022-10-05T19:01:59.886918Z","iopub.status.idle":"2022-10-05T19:02:14.824798Z","shell.execute_reply.started":"2022-10-05T19:01:59.886879Z","shell.execute_reply":"2022-10-05T19:02:14.823747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## <b>2 <span style='color:#3f4d63'>I</span> Data Preprocessing</b>\n---","metadata":{}},{"cell_type":"code","source":"import re\nfrom nltk.tokenize import sent_tokenize\nfrom textblob import TextBlob\n\ntrain['full_text'] = train[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex=True)\ntest['full_text'] = test[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex=True)\ntrain['ncharacters'] = train['full_text'].str.len()\ntrain['ncharacters'] = train['full_text'].str.len()\ntrain['sent_count'] = train['full_text'].apply(lambda x: len(sent_tokenize(x)))\ntrain['avg_sent_len'] = train['full_text'].apply(lambda x: np.mean([len(w.split()) for w in sent_tokenize(x)]))\ntrain['polarity'] = train['full_text'].apply(lambda x: TextBlob(x).sentiment[0])\ntrain['subjetivity'] = train['full_text'].apply(lambda x: TextBlob(x).sentiment[1])","metadata":{"_uuid":"e6257c35-615a-4c68-b926-0363d5b7f783","_cell_guid":"65b9a2e7-e64b-40ed-9bec-539ae9a13fa9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-05T19:02:14.826538Z","iopub.execute_input":"2022-10-05T19:02:14.827414Z","iopub.status.idle":"2022-10-05T19:02:41.104373Z","shell.execute_reply.started":"2022-10-05T19:02:14.827373Z","shell.execute_reply":"2022-10-05T19:02:41.103086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\n\ndef clean_text(text):\n    # Convert text to lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n    # Remove non-Roman characters\n    text = re.sub(\"([^\\x00-\\x7F])+\", \" \", text)    \n    return text","metadata":{"_uuid":"ff66a4be-0c90-4c0d-9c41-8030ace94712","_cell_guid":"3d55f096-92be-4448-be3c-ba6b220031e8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-05T19:02:41.105675Z","iopub.execute_input":"2022-10-05T19:02:41.106118Z","iopub.status.idle":"2022-10-05T19:02:41.120116Z","shell.execute_reply.started":"2022-10-05T19:02:41.106079Z","shell.execute_reply":"2022-10-05T19:02:41.117949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## <b>3 <span style='color:#3f4d63'>I</span> Modeling</b>\n---\n\n> **MUST read** -> [The art of pooling embeddings](https://blog.ml6.eu/the-art-of-pooling-embeddings-c56575114cf8)\n\nIn this section we're gonna use mainly **Deberta Transformer Model**. Its working procedure it's easy: \n\n* We feed the input sentence or text into a transformer network (like Bert, Deberta, etc). \n* The transformer produces contextualized word embeddings for all input tokens in our text.\n\nAs we want a fixed-sized output representation (vector u), we need a pooling layer. **Thus, in first subsections you'll find explanations for several type of poolings.** \n\n![](https://www.sbert.net/_images/SBERT_Architecture.png)","metadata":{"_uuid":"4e528e05-df6d-49ff-9a86-f63c89e6c061","_cell_guid":"827be9b5-8809-4304-a892-1f2ab3436ef1","trusted":true}},{"cell_type":"code","source":"from transformers import DebertaTokenizer, TFDebertaModel\n\ntokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\ntokenizer.save_pretrained('./tokenizer/')\nmodel = TFDebertaModel.from_pretrained(\"microsoft/deberta-base\")\nclear_output()","metadata":{"_uuid":"dcd65406-708c-4f37-85df-eb5984978a38","_cell_guid":"2dffa856-ed10-4c70-a4f3-4231088d4df5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-05T19:02:41.123677Z","iopub.execute_input":"2022-10-05T19:02:41.124183Z","iopub.status.idle":"2022-10-05T19:03:03.560688Z","shell.execute_reply.started":"2022-10-05T19:02:41.124137Z","shell.execute_reply":"2022-10-05T19:03:03.559556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Inputs Explained\n\nEvery model is different yet bears similarities with the others. Therefore most models use the same inputs, which are detailed here alongside usage examples. Let's explain a little bit what `tokenizer` is gonna return. **The tokenizer takes care of splitting the sequence into tokens available in the tokenizer vocabulary.**\n\n* Quick example:","metadata":{"_uuid":"203e9b3f-c6f0-4284-a29c-5bc2afce6a05","_cell_guid":"c98c6f7b-9b42-4ba0-bbaf-e2525e9be742","trusted":true}},{"cell_type":"code","source":"sequence = \"A Titan RTX has 24GB of VRAM\"\ntokenized_sequence = tokenizer.tokenize(sequence)\nprint(tokenized_sequence)\n\ndel sequence, tokenized_sequence\ngc.collect();","metadata":{"_uuid":"9b8b925c-65ab-44c1-a449-3f0c1507fb0c","_cell_guid":"01b2e419-75e9-4d53-a2b6-32529580016d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-05T19:03:03.562029Z","iopub.execute_input":"2022-10-05T19:03:03.562372Z","iopub.status.idle":"2022-10-05T19:03:03.832914Z","shell.execute_reply.started":"2022-10-05T19:03:03.562341Z","shell.execute_reply":"2022-10-05T19:03:03.831803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The tokens are either words or subwords. However, they must be **converted into IDs**, which are understandable by the model. \n\n> In this following [link](https://huggingface.co/docs/transformers/v4.22.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_encode_plus) you'll find information about `batch_encode_plus` and its parameters.","metadata":{"_uuid":"56f5e2d5-6e4f-45cf-be98-6fff5d7f2ab5","_cell_guid":"1e83ca1b-51c6-4028-937a-eb1da4464e56","trusted":true}},{"cell_type":"code","source":"# Inspired by: https://www.kaggle.com/code/junjitakeshima/ell-simple-roberta-starter-eng\ndef create_data(text, tokenizer):\n    '''Tokenizes the training data. It only returns IDs and attention masks.'''\n    \n    encoded = tokenizer.batch_encode_plus(text, add_special_tokens = True, max_length= 128,\n              padding='max_length', truncation=True, return_attention_mask=True)\n\n    input_ids       = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n    attention_mask = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n\nmodel_inputs = create_data(train['full_text'], tokenizer)\ntest_data = create_data(test['full_text'], tokenizer)\nmodel_inputs","metadata":{"_uuid":"f15c43fc-308c-40aa-b114-b1c85d95d963","_cell_guid":"f1172d0c-a77f-43b3-84af-7ac813b63417","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-05T19:03:03.834144Z","iopub.execute_input":"2022-10-05T19:03:03.834455Z","iopub.status.idle":"2022-10-05T19:03:25.752537Z","shell.execute_reply.started":"2022-10-05T19:03:03.834427Z","shell.execute_reply":"2022-10-05T19:03:25.75062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explanation**:\n\n* **Input IDs**: they are often the **only required parameters** to be passed to the **model** as **input**. They are **token indices**, numerical representations of tokens building the sequences that will be used as input by the model. For more detailed information watch [this video](https://youtu.be/VFp38yj8h3A).\n\n* **Attention Mask**: this argument indicates to the model which tokens should be attended to, and which should not. For example, consider these two sequences:","metadata":{"_uuid":"258a188e-629b-4e9c-ac73-a8433bc752f4","_cell_guid":"003239c0-22a3-44c2-8477-36f4a756a4c7","trusted":true}},{"cell_type":"code","source":"sequence_a = \"This is a short sequence.\"\nsequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n\nencoded_sequence_a = tokenizer(sequence_a)[\"input_ids\"]\nencoded_sequence_b = tokenizer(sequence_b)[\"input_ids\"]","metadata":{"_uuid":"95c96ca0-c490-47ed-842e-0961b4d00521","_cell_guid":"69d80788-5422-4632-9388-59f428668a47","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-05T19:03:25.756019Z","iopub.execute_input":"2022-10-05T19:03:25.756622Z","iopub.status.idle":"2022-10-05T19:03:25.765302Z","shell.execute_reply.started":"2022-10-05T19:03:25.756562Z","shell.execute_reply":"2022-10-05T19:03:25.763894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The encoded versions have different lengths:","metadata":{"_uuid":"6c46215c-ea0e-4acd-ad99-5ab705e5b1b6","_cell_guid":"b56d9e62-1113-4f9c-8e80-c65dbc65286f","trusted":true}},{"cell_type":"code","source":"print(len(encoded_sequence_a), len(encoded_sequence_b))\n\ndel sequence_a, sequence_b, encoded_sequence_a, encoded_sequence_b\ngc.collect();","metadata":{"_uuid":"5f9c0ded-11fe-4213-9573-07387345d67f","_cell_guid":"2c4e9d0d-a9c8-43f2-8b4f-dc22c5ec4689","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-05T19:03:25.769161Z","iopub.execute_input":"2022-10-05T19:03:25.769519Z","iopub.status.idle":"2022-10-05T19:03:26.26786Z","shell.execute_reply.started":"2022-10-05T19:03:25.769488Z","shell.execute_reply":"2022-10-05T19:03:26.266298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Therefore, we can’t put them together in the same tensor as-is. The first sequence needs to be padded up to the length of the second one, or the second one needs to be truncated down to the length of the first one.\n\nIn the first case, the list of IDs will be extended by the padding indices. This is the moment where the attention mask comes into play. **The attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them.**\n\n## **Model Outputs Explained**\n\nThis section is mainly inspired in these amazing works: [Utilizing Transformer Representations Efficiently](https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently), and [Illustrated Bert](http://jalammar.github.io/illustrated-bert/). Its aim is to explain commonly transformer's outputs. This will help to understand next sections, dedicated to pooling embeddings. ","metadata":{"_uuid":"6f21b1a8-be27-4d98-b94c-125b9d395ed4","_cell_guid":"6368af1c-af2f-47f4-8c07-1bf5150c5aac","trusted":true}},{"cell_type":"code","source":"# Code taken from: https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently\n\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)\nlogging.set_verbosity_error()\nlogging.set_verbosity_warning()\n\ntrain_text = train['full_text'][:16].tolist()\nmax_seq_length = 256\n_pretrained_model = 'roberta-base'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nmodel2 = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer2 = AutoTokenizer.from_pretrained(_pretrained_model)\n\nclear_output()\n\nfeatures = tokenizer2.batch_encode_plus(\n    train_text,\n    add_special_tokens=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    truncation=True,\n    return_tensors='pt',\n    return_attention_mask=True\n)\n\nwith torch.no_grad():\n    outputs = model2(features['input_ids'], features['attention_mask'])\noutputs","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-10-05T19:03:26.269546Z","iopub.execute_input":"2022-10-05T19:03:26.269958Z","iopub.status.idle":"2022-10-05T19:04:07.27593Z","shell.execute_reply.started":"2022-10-05T19:03:26.269922Z","shell.execute_reply":"2022-10-05T19:04:07.27491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explanations**\n\n* `last_hidden_state`: this is the first and default output from models. It's a sequence of hidden-states at the output of the last layer of the model. The `last_hidden_state` is **passed into the pooler** and this returns the `pooler_output`. We can deactivate pooler outputs by setting add pooling layer to False in model config and passing that to model.\n\n* `pooler_output`: last layer hidden-state of the first token of the sequence (classification token) after further processing by a Linear layer and a Tanh activation function.\n\n* `hidden_states`: hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n\n* `attentions`\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/20200420231335/elmo-eemmbeddings.jpg)\n\n## **CLS Embeddings**\n\n[CLS] token appears at the very beginning of each sentence, it has a fixed embedding and a fix positional embedding, thus this token contains no information itself. However, **the output of [CLS] is inferred by all other words in this sentence**, so [CLS] token would have captured the entire context. This makes [CLS] a good representation for sentence-level classification.\n\n![](https://help-static-aliyun-doc.aliyuncs.com/assets/img/en-US/4024896061/p162090.png)\n\n* **Implementation**: ","metadata":{}},{"cell_type":"code","source":"last_hidden_state = outputs[0]\ncls_embeddings = last_hidden_state[:, 0]\n#logits = nn.Linear(config.hidden_size, 1)(cls_embeddings) # regression head","metadata":{"execution":{"iopub.status.busy":"2022-10-05T19:04:07.279321Z","iopub.execute_input":"2022-10-05T19:04:07.281972Z","iopub.status.idle":"2022-10-05T19:04:07.287828Z","shell.execute_reply.started":"2022-10-05T19:04:07.281935Z","shell.execute_reply":"2022-10-05T19:04:07.286693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Types of Pooling**\n\n### **Mean Pooling**\n\nThere are different ways of procceding. We can simply average all contextualized word embeddings BERT is giving us. However, we'll make it a bit more complicated. We will make use of attention masks as well so that we can ignore padding tokens which is a better way of implementing average embeddings.\n\n* **Step 1**: Expand Attention Mask from [batch_size, max_len] to [batch_size, max_len, hidden_size].\n* **Step 2**: Sum Embeddings along max_len axis so now we have [batch_size, hidden_size].\n* **Step 3**: Sum Mask along max_len axis. This is done so that we can ignore padding tokens.\n* **Step 4**: Take Average.","metadata":{}},{"cell_type":"code","source":"attention_mask = features['attention_mask']\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\nsum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\nsum_mask = input_mask_expanded.sum(1)\nsum_mask = torch.clamp(sum_mask, min=1e-9)\nmean_embeddings = sum_embeddings / sum_mask\n#logits = nn.Linear(config.hidden_size, 1)(mean_embeddings) # regression head","metadata":{"execution":{"iopub.status.busy":"2022-10-05T19:04:07.289074Z","iopub.execute_input":"2022-10-05T19:04:07.28941Z","iopub.status.idle":"2022-10-05T19:04:07.377479Z","shell.execute_reply.started":"2022-10-05T19:04:07.289379Z","shell.execute_reply":"2022-10-05T19:04:07.376432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Max Pooling**\n\nThis is quite similar to the previous type. The difference between them is, as its name reflects, that rather than performing the mean operation, we perform the maximum one. ","metadata":{}},{"cell_type":"code","source":"input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\nlast_hidden_state[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\nmax_embeddings = torch.max(last_hidden_state, 1)[0]","metadata":{"execution":{"iopub.status.busy":"2022-10-05T19:04:07.378715Z","iopub.execute_input":"2022-10-05T19:04:07.379039Z","iopub.status.idle":"2022-10-05T19:04:07.46472Z","shell.execute_reply.started":"2022-10-05T19:04:07.379009Z","shell.execute_reply":"2022-10-05T19:04:07.463728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are more types of pooling, but for not making this notebook so tedious I'm just gonna finish here. If you are interested, [here](https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently) you'll find more types.","metadata":{}},{"cell_type":"code","source":"del train_text, tokenizer2, model2, config\ngc.collect();","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-10-05T19:04:07.466395Z","iopub.execute_input":"2022-10-05T19:04:07.4668Z","iopub.status.idle":"2022-10-05T19:04:07.793049Z","shell.execute_reply.started":"2022-10-05T19:04:07.466765Z","shell.execute_reply":"2022-10-05T19:04:07.791698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model's Architecture (no finetuning)\n\nRegarding previous versions' performance we conclude that we must speed up training and inference process. We can't spent such a big amount of time training **ONLY ONE TRANSFORMER-BASED MODEL**. Imagine what will happen if we try to make an ensembling !\n\nTherefore, we need to change the approach a bit. In this subsection we're gonna focus on extracting embedding from **non-finetuned** Transformers (Deberta base, Deberta Large, Funnel Transformer, Bigbird, etc). With this, we'll save a huge amount of time. Lastly, having all this done we'll add a multioutpt regressor as the head of our model, and feed it with an ensembling of all those embedding we extracted before. \n\n* One **advantage** that this approach has is that as well as embeddings, we can feed the regressor with features like (sentence count, polarity, subjetivity). In other words, we can make some **feature engineering**. ","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output, display_html\nimport gc; gc.enable()\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\nimport warnings\nfrom pathlib import Path\n\n# Basic libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy as sc\nfrom scipy import stats\n\n# Train Test Split\nfrom sklearn.model_selection import train_test_split\n\n# Cross Validation\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, learning_curve, train_test_split\n\n# Optuna - Bayesian Optimization \nimport optuna\nfrom optuna.samplers import TPESampler\n\n# Tensorflow\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# PyTorch\nimport torch\nimport torch.nn as nn\n\n# Transformers\nimport transformers\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\n\n# Regressors\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom tqdm import tqdm\n\nwarnings.filterwarnings('ignore')\nMAX_SEQ_LEN = 256\n\ndef load_data():\n    '''Load each of the datasets we are given.'''\n    \n    data_dir = Path(\"../input/feedback-prize-english-language-learning\")\n    train = pd.read_csv(data_dir / \"train.csv\")\n    test = pd.read_csv(data_dir / \"test.csv\")\n    sample_submission = pd.read_csv(data_dir / 'sample_submission.csv')\n    return train, test, sample_submission\n\ndef mcrmse(y_true, y_pred):\n    '''Returns value for MCRMSE (competition's metric)'''\n    \n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=0)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=0)\n\ntrain, test, sample_submission = load_data()\nclear_output()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-27T15:59:41.133636Z","iopub.execute_input":"2022-09-27T15:59:41.134007Z","iopub.status.idle":"2022-09-27T15:59:41.227107Z","shell.execute_reply.started":"2022-09-27T15:59:41.133975Z","shell.execute_reply":"2022-09-27T15:59:41.225963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\n\n#Aquí hay un problema, al borrar las comas si no hay espacio con la palabra siguiente\n#se nos juntan las palabras !!!\n# Ejemplo: casa,perro,gato\n\ndef clean_text(text):\n    # Convert text to lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n    # Remove non-Roman characters\n    text = re.sub(\"([^\\x00-\\x7F])+\", \" \", text)    \n    return text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['full_text'] = train['full_text'].map(lambda x: clean_text(x))\ntrain['full_text'] = train[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex=True)\n\ntest['full_text'] = test['full_text'].map(lambda x: clean_text(x))\ntest['full_text'] = test[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we know, each token from the text will have an embedding. However, we want just **one embedding per essay**. In order to get it, we need perform a **pooling operation**. There are a wide variety of types. Hereafter, you'll find some of them implemented. ","metadata":{}},{"cell_type":"code","source":"def CLS_embeddings(output):\n    '''Returns the embeddings corresponding to the <CLS> token of each text. '''\n\n    last_hidden_state = output[0]\n    cls_embeddings = last_hidden_state[:, 0]\n    return cls_embeddings\n\ndef MeanPooling(output, attention_mask): \n    '''Performs the mean pooling operation. '''\n    \n    last_hidden_state = output[0]\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n    sum_mask = input_mask_expanded.sum(1)\n    sum_mask = torch.clamp(sum_mask, min=1e-9)\n    mean_embeddings = sum_embeddings / sum_mask\n    return mean_embeddings\n\ndef MaxPooling(output, attention_mask):\n    '''Performs the max pooling operation. '''\n        \n    last_hidden_state = output[0]\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n    last_hidden_state[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n    max_embeddings = torch.max(last_hidden_state, 1)[0]\n    return max_embeddings","metadata":{"execution":{"iopub.status.busy":"2022-09-27T15:59:47.079325Z","iopub.execute_input":"2022-09-27T15:59:47.079687Z","iopub.status.idle":"2022-09-27T15:59:47.088522Z","shell.execute_reply.started":"2022-09-27T15:59:47.079656Z","shell.execute_reply":"2022-09-27T15:59:47.087465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Due to kernel's memory limit, we'll need to split our data to get the embeddings. To do so, we'll make use of `DataLoader` from torch utils. We're gonna a `batch_size` of 4.\n\n> Try to tokenize the whole dataset texts at once. You'll find that when trying to get the embeddings from the model the notebook will run out of memory. Due to that we proceed as told above. ","metadata":{}},{"cell_type":"code","source":"train_data_loader = torch.utils.data.DataLoader(train.full_text.tolist(),\\\n                        batch_size=4,\\\n                        shuffle=False)\n\ntest_data_loader = torch.utils.data.DataLoader(test.full_text.tolist(),\\\n                        batch_size=4,\\\n                        shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T15:59:49.912077Z","iopub.execute_input":"2022-09-27T15:59:49.91249Z","iopub.status.idle":"2022-09-27T15:59:49.920779Z","shell.execute_reply.started":"2022-09-27T15:59:49.912457Z","shell.execute_reply":"2022-09-27T15:59:49.919694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, let's define the functions to get our text's embeddings. ","metadata":{}},{"cell_type":"code","source":"def get_embeddings_from(data, tokenizer, model, pooler):\n    '''Returns an embedding list for a concrete data. '''\n    \n    final_embeddings = []    \n    for batch in tqdm(data, total = len(data)):\n        model_inputs = tokenizer.batch_encode_plus(\n            batch,\n            add_special_tokens=True,\n            padding='max_length',\n            max_length=MAX_SEQ_LEN,\n            truncation=True,\n            return_tensors='pt',\n            return_attention_mask=True\n        )\n\n        #input_ids = model_inputs['input_ids']\n        #attention_mask = model_inputs['attention_mask']\n        input_ids = model_inputs['input_ids'].to(\"cuda\")\n        attention_mask = model_inputs['attention_mask'].to(\"cuda\")\n        with torch.no_grad():\n              outputs = model(input_ids, attention_mask)\n\n        # We want one embedding per text. Thus, we need to perform a pooling operation\n        if pooler == 'max':\n            embeddings = MaxPooling(outputs, attention_mask)\n        elif pooler == 'mean':\n            embeddings = MeanPooling(outputs, attention_mask)\n        else:\n            embeddings = CLS_embeddings(outputs)\n            \n        embeddings = embeddings.cpu().detach().numpy()\n        final_embeddings.extend(embeddings)\n        \n    return final_embeddings\n\ndef extract_embeddings(_pretrained_model, pooler):\n    '''Downloads model and tokenizer from _pretrained_model. \n       It returns embeddings for each training and testing data.'''\n    \n    #config = AutoConfig.from_pretrained(_pretrained_model)\n    model = AutoModel.from_pretrained(_pretrained_model)\n    tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\n    # We are gonna sent everything to GPU, to make the extraction even faster\n    model = model.to(\"cuda\")\n    train_embeddings = get_embeddings_from(train_data_loader, tokenizer, model, pooler)\n    test_embeddings = get_embeddings_from(test_data_loader, tokenizer, model, pooler)\n        \n    return train_embeddings, test_embeddings","metadata":{"execution":{"iopub.status.busy":"2022-09-27T15:59:51.666363Z","iopub.execute_input":"2022-09-27T15:59:51.666743Z","iopub.status.idle":"2022-09-27T15:59:51.676711Z","shell.execute_reply.started":"2022-09-27T15:59:51.666694Z","shell.execute_reply":"2022-09-27T15:59:51.675424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transformers that we're going to use to get the embeddings: \n\n* DeBERTa Large\n* DeBERTa v3 Base\n* DeBERTa v3 Large\n* Big Bird (RoBERTa based)\n* Funnel Transformer","metadata":{}},{"cell_type":"code","source":"for pooler in ['max','mean','cls']:\n    deberta_large_train_embeddings, deberta_large_test_embeddings = extract_embeddings('../input/huggingface-deberta-variants/deberta-large/deberta-large', pooler)\n    debertav3_train_embeddings, debertav3_test_embeddings = extract_embeddings('../input/huggingface-deberta-variants/deberta-base/deberta-base', pooler)\n    debertav3_large_train_embeddings, debertav3_large_test_embeddings = extract_embeddings('../input/deberta-v3-large/deberta-v3-large', pooler)\n    bigbird_train_embeddings, bigbird_test_embeddings = extract_embeddings('../input/huggingfacebigbirdrobertabase', pooler)\n    funnel_train_embeddings, funnel_test_embeddings = extract_embeddings('../input/funneltransformerlarge', pooler)\n    \n    transformers_train_embeddings = [deberta_large_train_embeddings, debertav3_train_embeddings,\n            debertav3_large_train_embeddings, bigbird_train_embeddings, funnel_train_embeddings]\n\n    transformers_test_embeddings = [deberta_large_test_embeddings, debertav3_test_embeddings,\n                debertav3_large_test_embeddings, bigbird_test_embeddings, funnel_test_embeddings]\n\n    if pooler == 'max':\n        train_embeddings_np = np.concatenate(transformers_train_embeddings, axis = 1)\n        test_embeddings_np = np.concatenate(transformers_test_embeddings, axis = 1)\n        train_embeddings = pd.DataFrame(train_embeddings_np)\n        test_embeddings = pd.DataFrame(test_embeddings_np)\n        \n    else: \n        train_embeddings = pd.concat([train_embeddings, pd.Series(np.concatenate(transformers_train_embeddings, axis = 1))], axis = 1)\n        test_embeddings = pd.concat([test_embeddings, pd.Series(np.concatenate(transformers_test_embeddings, axis = 1))], axis = 1)\n\n    # In order to save some memory let's delete each transformer's embeddings\n    for dt in [transformers_train_embeddings, transformers_test_embeddings]:\n        for emb in dt: \n            del emb","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-09-27T15:59:55.411944Z","iopub.execute_input":"2022-09-27T15:59:55.412419Z","iopub.status.idle":"2022-09-27T16:08:28.190195Z","shell.execute_reply.started":"2022-09-27T15:59:55.412376Z","shell.execute_reply":"2022-09-27T16:08:28.188992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once we have extracted all the embeddings we wanted, it's time to concat them into one huge embedding. Apart from that, we can get rid of each transformer embeddings. By doing this we'll save some useful memory.  \n\n### **PCA**\n\nThe embeddings dataframes that we have extracted are ridiculously large. However, we can perform some dimensionality reduction to fix it. We'll proceed with Principal Component Analysis technique: \n\n> Please note that, although here you'll not find anything about that here, pre-requisites (for a proper PCA application) such as correlation between features, normalized data, etc have been checked already. ","metadata":{}},{"cell_type":"code","source":"#train_embeddings = pd.DataFrame(train_embeddings_np)\n#test_embeddings = pd.DataFrame(test_embeddings_np)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import *\nfrom sklearn.decomposition import PCA\n\ndef apply_pca(X, transformer = False, components = -1):\n    aux = X.copy()\n    if transformer:\n        X = pd.DataFrame(transformer.fit_transform(X))\n        X.columns = aux.columns    \n    # Create principal components\n    if components == -1:\n        pca = PCA()\n    else:\n        pca = PCA(n_components = components)\n        \n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2, sharey = True)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\")    # ylim = (0.0, 1.0) o sino sharey en plt.subplots\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\"\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_para_pca = train_embeddings\nsc = StandardScaler()\nX_para_pca = pd.DataFrame(sc.fit_transform(X_para_pca))\npca, X_pca, loadings = apply_pca(X_para_pca)\nplt.figure(figsize = (22,5))\nax0, ax1 = plot_variance(pca)\nax1.set_xlim(-50,2000)\nax1.axhline(y = 0.9, color = 'r', linestyle = '-') ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the previous chart we can't guess clearly which is the optimal number of components to choose. There are some techniques to discover it. In this case, the rule of Abdi et al. (2010) is used. **The variances explained by the principal components are averaged and we select those whose proportion of explained variance exceeds the mean.** ","metadata":{}},{"cell_type":"code","source":"#np.mean(pca.explained_variance_ratio_, axis = 1)\nmedia = np.mean(pca.explained_variance_ratio_)\ncomponentes = []\nfor i in range(X_pca.shape[1]):\n    if pca.explained_variance_ratio_[i] > media: \n        componentes.append(i)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"component = 'PC{}'.format(len(componentes))\ntrain_embeddings = X_pca.loc[:,:component]\ntest_embeddings = sc.transform(test_embeddings)\ntest_embeddings = pca.transform(test_embeddings)\ntest_embeddings = pd.DataFrame(test_embeddings, columns = ['PC{}'.format(i) for i in range(1,X_pca.shape[1]+1)]).loc[:,:component]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Applying Regressors**\n\nFrom now onwards, there's a big variety of possible configurations. Starting from the regressors to use, to feature engineering, the different kind of cross validation techniques to apply, the ensembling part, etc. It's a personal choice. \n\n* In this version you'll find some hyperparameter tuning done with Optuna, for both XGBoost and LightGBM.  ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error as mae\ndef objective(trial):\n    # Parámetros XGBoost\n    params = {\"random_state\":trial.suggest_categorical(\"random_state\", [2022]),           # categorical for concrete values\n        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),   # loguniform for continuos values\n        \"n_estimators\": trial.suggest_int('n_estimators',50,2000),                 # int for discrete values. Interval between [100,2000]\n        \"max_depth\" : trial.suggest_int(\"max_depth\", 1, 20),\n        \"alpha\" : trial.suggest_loguniform('alpha',0.9,1),\n        \"tree_method\": trial.suggest_categorical(\"tree_method\", ['gpu_hist']), \n        \"predictor\": trial.suggest_categorical(\"predictor\", [\"gpu_predictor\"])\n    }\n\n    model = XGBRegressor(**params)\n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n    model.fit(\n        X_train_tmp, y_train_tmp,\n        eval_set=[(X_valid_tmp, y_valid_tmp)],\n        early_stopping_rounds=35, verbose=0\n    )\n        \n    y_train_pred = model.predict(X_train_tmp)\n    y_valid_pred = model.predict(X_valid_tmp)\n    train_mae = mae(y_train_tmp, y_train_pred)\n    valid_mae = mae(y_valid_tmp, y_valid_pred)\n    \n    print(f'MCRMSE of Train: {train_mae}')\n    print(f'MCRMSE of Validation: {valid_mae}')\n    \n    return valid_mae\n\ndef optuna_xgb(trials = 100, timeout = 1200):\n    TRIALS = trials\n    TIMEOUT = timeout\n    \n    sampler = TPESampler(seed=42)\n\n    study = optuna.create_study(\n        study_name = 'XGB_parameter_opt',\n        direction = 'minimize',\n        sampler = sampler,\n    )\n    study.optimize(objective, n_trials=TRIALS)\n    print(\"Best Score:\",study.best_value)\n    print(\"Best trial\",study.best_trial.params)\n    \n    best_params = study.best_params\n    return best_params\n\ndef objective_lgbm(trial):\n    # Parámetros LightGBM\n    params = {\"random_state\":trial.suggest_categorical(\"random_state\", [2022]),           \n        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),   \n        \"n_estimators\": trial.suggest_int('n_estimators',50,2000),                 \n        \"max_depth\" : trial.suggest_int(\"max_depth\", 1, 20),\n        \"_min_child_weight\" : trial.suggest_float(\"_min_child_weight\", 0.1, 10),\n        \"reg_lambda\" : trial.suggest_float(\"reg_lambda\", 0.01, 10),\n        \"reg_alpha\" : trial.suggest_float('reg_alpha',0.01,10),\n        \"num_leaves\" : trial.suggest_int(\"num_leaves\", 50, 100),\n        'subsample' : trial.suggest_float('subsample', 0.01, 1), \n        'device': trial.suggest_categorical(\"device\", ['gpu'])\n    }\n\n    model = LGBMRegressor(**params)\n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n    model.fit(\n        X_train_tmp, y_train_tmp,\n        eval_set=[(X_valid_tmp, y_valid_tmp)],\n        early_stopping_rounds=35, verbose=0\n    )\n        \n    y_train_pred = model.predict(X_train_tmp)\n    y_valid_pred = model.predict(X_valid_tmp)\n    train_mae = mae(y_train_tmp, y_train_pred)\n    valid_mae = mae(y_valid_tmp, y_valid_pred)\n    \n    print(f'MCRMSE of Train: {train_mae}')\n    print(f'MCRMSE of Validation: {valid_mae}')\n    \n    return valid_mae\n\ndef optuna_lgbm(trials = 100, timeout = 1200):\n    TRIALS = trials\n    TIMEOUT = timeout\n    \n    sampler = TPESampler(seed=42)\n\n    study = optuna.create_study(\n        study_name = 'LGBM_parameter_opt',\n        direction = 'minimize',\n        sampler = sampler,\n    )\n    study.optimize(objective_lgbm, n_trials=TRIALS)\n    print(\"Best Score:\",study.best_value)\n    print(\"Best trial\",study.best_trial.params)\n    \n    best_params = study.best_params\n    return best_params","metadata":{"execution":{"iopub.status.busy":"2022-09-27T15:56:08.405632Z","iopub.status.idle":"2022-09-27T15:56:08.406482Z","shell.execute_reply.started":"2022-09-27T15:56:08.406229Z","shell.execute_reply":"2022-09-27T15:56:08.406254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX = train_embeddings\nscores = train.select_dtypes([int, float]).columns[:6]\ndef model_preds(regressor, optuna_tuning):\n    '''Returns predictions made by a concrete regressor. '''\n    global y\n    preds = {}\n    for j, col in enumerate(scores): \n        y = train[col]\n        best_params = optuna_tuning()\n        regressor_tuned = regressor(**best_params)\n        \n        regressor_tuned.fit(train_embeddings, train[col])  \n        preds[j] = regressor_tuned.predict(test_embeddings)\n\n    return preds\n\nxgb_preds = model_preds(XGBRegressor, optuna_xgb)\nlgbm_preds = model_preds(LGBMRegressor, optuna_lgbm)\nclear_output()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = {}\nfor i in range(6):\n    preds[i] = (xgb_preds[i] + lgbm_preds[i]) / 2","metadata":{"execution":{"iopub.status.busy":"2022-09-27T15:56:08.407914Z","iopub.status.idle":"2022-09-27T15:56:08.408978Z","shell.execute_reply.started":"2022-09-27T15:56:08.40868Z","shell.execute_reply":"2022-09-27T15:56:08.408708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**To be continued ...**","metadata":{"_uuid":"081323a9-6ada-43bd-8999-7f6de032286b","_cell_guid":"3403fcb8-f784-4ad4-bff3-0db18cd38c24","trusted":true}}]}